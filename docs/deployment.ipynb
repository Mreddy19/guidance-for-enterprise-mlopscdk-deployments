{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0048193f",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "At this step we will demonstrate how to transition from experimenting in SageMaker Studio notebook to pushing the model to production. We will show how to approve the model version in the staging environment and then transition it to a production environment. The example below focuses on the real-time inference case.\n",
    "\n",
    "These are the main steps that will be undertaken by the data scientist to deploy a model using the MLOPs platform:\n",
    "\n",
    "1. Update the configuration file in SageMaker Studio to define how the model will be built.\n",
    "2. Commit your changes in the project directory in SageMaker Studio to a local development branch. \n",
    "3. Push the local development branch to the Code Commit repository associated with the project.\n",
    "3. Create a merge request to merge the development branch with the main branch. This will initiate a Code Pipeline to build the model using SageMaker Pipelines.\n",
    "4. Approve model version in SageMaker Studio. This will initiate a Code Pipeline to deploy the model in the staging environment. Note, that in the multi-account deployment the staging environment will be a separate AWS account. In the single account deployment the staged model will be deployed in the same account. \n",
    "5. Approve deployment to production in Code Pipeline to create a production endpoint. Note, that in the multi-account deployment the production environment will be a separate AWS account. In the single account deployment the model in production will be deployed in the same account. \n",
    "\n",
    "\n",
    "Make sure to log into AWS Console with the Data Science User role to perform the steps below. Follow [AWS console](https://signin.aws.amazon.com/switchrole) link to assume the Data Science Admin role. The role has been created for you during the MLOps platform deployment. The role name has the following pattern:  **region-datascientist-account_id**. You might find it helpful to first navigate to IAM in AWS Console in order to retrieve the role name and then assume it using [AWS console](https://signin.aws.amazon.com/switchrole) link.\n",
    "\n",
    "## Trigger Model Building\n",
    "\n",
    "When a data scientist completed model experimentation and is ready to push it to production, they need to modify a parameter file associated with the project to provide the key information about the model and the data necessary for building the model. \n",
    "\n",
    "1. During the **Model Development** stage you were asked to take a note of the project S3 bucket. This information can also be retrieved by running the command `cat .project_params.yaml` in SageMaker Studio terminal.\n",
    "\n",
    "2. Once the S3 bucket name is retrieved, modify the `model_config.yaml` by replacing the placeholder in the file for the S3 bucket under data_config/inputs/data. At this point the data scientist has been working with files in the cloned project repository. Ensure that you modify the `model_config.yaml` in your project repository. \n",
    "\n",
    "![ds_model_build2.png](images/ds_model_build.gif)\n",
    "\n",
    "This file `model_config.yaml` defines how the model training pipeline is configured and the parameters in the file map to SageMaker Python SDK. The mapping is outlined in the table below:\n",
    "\n",
    "<center>\n",
    "\n",
    "model_config.yaml parameter | SageMaker Python SDK\n",
    "------------------------------|---------------------\n",
    "`preprocess_config` | [Sagemaker Preprocessing job](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ScriptProcessor)\n",
    "`train_config` | [Sagemaker training job](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator)\n",
    "`inference_config` | [Sagemaker Endpoint](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html?highlight=RegisterModel#sagemaker.workflow.step_collections.RegisterModel)\n",
    "`framework`, `framework_version` | used for finding the base docker images<br />for ML framework, such as sklearn, pytorch, tensorflow, etc. <br />Refer to [Image URIs](https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html)\n",
    "`data_config` | [data input](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.processing.ProcessingInput)\n",
    "`bias_config` | [bias config](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html?highlight=BiasConfig#sagemaker.clarify.BiasConfig)\n",
    "\n",
    "</center>\n",
    "\n",
    "3. Once the changes to the `model_config.yaml` are made, you can commit your changes to the `dev` branch, and push the branch to the Code Commit repository associated with the project.\n",
    "\n",
    " ![ds_model_build4.png](images/ds_model_build4.gif)\n",
    "\n",
    "4. Navigate to the [Code Commit console](https://console.aws.amazon.com/codecommit/home) page and click on `Create pull Request` from the `dev` (source) to the `main` (destination) branch. \n",
    "\n",
    "![ds_model_build5.png](images/ds_model_build5.png)\n",
    "\n",
    "![ds_model_build6.png](images/ds_model_build6.png)\n",
    "\n",
    "Provide merge title and an optional description. \n",
    "\n",
    "![ds_model_build7.png](images/ds_model_build7.png)\n",
    "\n",
    "Once the branches are merged successfully it will automatically trigger the model building via using Code Pipeline and SageMaker Pipelines.\n",
    "\n",
    "![ds_model_build8.png](images/ds_model_build8.png)\n",
    "\n",
    "![ds_model_build9.png](images/ds_model_build9.png)\n",
    "\n",
    "5. The status of the Code Pipeline can be monitored by navigating to [Code Pipeline Console](https://console.aws.amazon.com/codepipeline/home). The name of the pipeline is **projectname_build_pipeline**. \n",
    "\n",
    "![ds_model_build10.png](images/ds_model_build10.png)\n",
    "\n",
    "![ds_model_build11.png](images/ds_model_build11.png)\n",
    "\n",
    "6. The model is built using SageMaker Pipelines. The progress of this step can be monitored in SageMaker Studio as shown below: \n",
    "\n",
    "![ds_model_build12.gif](images/ds_model_build12.gif)\n",
    "\n",
    "## Approve Model Deployment\n",
    "\n",
    "1. Once the model building pipeline is completed, the new model will be available through SageMaker Studio. You can navigate to SageMaker resources and select `Model Registry`. In the `Model Registry` you will be able to click on the `Model group name` that corresponds to your project. This will show all model versions available for this project together. \n",
    "\n",
    "2. Initial model status will be `Pending`. At this stage you can update the status to `Approved`. This action will trigger execution of another Code Pipeline **projectname_deploy_pipeline**. The purpose of this execution is to deploy the model to a staging environment. Please note, that in multi-account MLOPs deployment the staging environment will be a separate AWS account. \n",
    "\n",
    "![ds_model_deployment.gif](images/ds_model_deployment.gif)\n",
    "\n",
    "3. To monitor the status of **projectname_deploy_pipeline** Code Pipeline navigate to the [Code Pipeline Console](https://console.aws.amazon.com/codepipeline/home).\n",
    "\n",
    "![ds_model_deployment5.png](images/ds_model_deployment5.png)\n",
    "\n",
    "![ds_model_deployment6.png](images/ds_model_deployment6.png)\n",
    "\n",
    "![ds_model_deployment7.png](images/ds_model_deployment7.png)\n",
    "\n",
    "4. Once the pipeline execution is completed the staging endpoint **projectname-staging-endpoint** will be `InService`. The status of the endpoint can be verified by navigating to SageMaker console, `Inference` and then `Endpoints`. \n",
    "\n",
    "![ds_model_deployment8.png](images/ds_model_deployment8.png)\n",
    "\n",
    "5. Next, the model can be promoted to the production environment. In the multi-account deployment it will be a different AWS account from the staging account. This is accomplished through a manual approval step in the execution of **projectname_deploy_pipeline** Code Pipeline which at this stage is paused in the `ApproveDeployment` state. The data scientist will click on the `Review` button in the step `ApproveDeployment` and optionally provide comments. This action will un-pause the Code Pipeline execution and result in the deployment of the production endpoint with the model monitoring enabled. \n",
    "\n",
    "![ds_model_deployment9.png](images/ds_model_deployment9.png)\n",
    "\n",
    "![ds_model_deployment10.png](images/ds_model_deployment10.png)\n",
    "\n",
    "6. The status of the production endpoint can be monitored by navigating to SageMaker console, `Inference` and then `Endpoints`. The name of the production endpoint is **project-prod-endpoint**.\n",
    "\n",
    "![ds_model_deployment11.png](images/ds_model_deployment11.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
